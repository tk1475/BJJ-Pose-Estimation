{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BJJ position detection with multi-person support...\n",
      "Choose detection method:\n",
      "1: Basic Multi-Person Detection\n",
      "2: Holistic-based Multi-Person Detection\n",
      "3: Optimized Multi-Person Detection (Recommended)\n",
      "Model loaded successfully from pose_transformer.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746550441.284911 43561978 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "I0000 00:00:1746550441.292908 43561978 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1746550441.412176 43582043 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746550441.412183 43582034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746550441.440229 43582034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746550441.441353 43582046 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection stopped by user\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "POSITION_CLASSES = [\n",
    "    'standing', \n",
    "    'takedown1', 'takedown2',\n",
    "    'open_guard1', 'open_guard2',\n",
    "    'half_guard1', 'half_guard2',\n",
    "    'closed_guard1', 'closed_guard2',\n",
    "    '50-50_guard',\n",
    "    'side_control1', 'side_control2',\n",
    "    'mount1', 'mount2',\n",
    "    'back1', 'back2',\n",
    "    'turtle1', 'turtle2'\n",
    "]\n",
    "\n",
    "class PoseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=34, embed_dim=64, num_heads=4, num_classes=18, dropout=0.1):\n",
    "        super(PoseTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=128,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(embed_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.embedding(x)  \n",
    "        x = self.transformer_encoder(x) \n",
    "        out = self.classifier(x) \n",
    "        return out\n",
    "\n",
    "def load_model(model_path='pose_transformer.pth'):\n",
    "    model = PoseTransformer()\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def setup_mediapipe_multi():\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    return mp_pose, pose\n",
    "\n",
    "def preprocess_keypoints(landmarks, image_height=480, image_width=640):\n",
    "    if not landmarks:\n",
    "        return None\n",
    "    \n",
    "    mp_to_coco = {\n",
    "        0: 0,    # nose\n",
    "        2: 1,    # left eye\n",
    "        5: 2,    # right eye\n",
    "        7: 3,    # left ear\n",
    "        8: 4,    # right ear\n",
    "        11: 5,   # left shoulder\n",
    "        12: 6,   # right shoulder\n",
    "        13: 7,   # left elbow\n",
    "        14: 8,   # right elbow\n",
    "        15: 9,   # left wrist\n",
    "        16: 10,  # right wrist\n",
    "        23: 11,  # left hip\n",
    "        24: 12,  # right hip\n",
    "        25: 13,  # left knee\n",
    "        26: 14,  # right knee\n",
    "        27: 15,  # left ankle\n",
    "        28: 16   # right ankle\n",
    "    }\n",
    "    \n",
    "    coco_keypoints = []\n",
    "    for mp_idx, coco_idx in mp_to_coco.items():\n",
    "        lm = landmarks[mp_idx]\n",
    "        x, y = lm.x * image_width, lm.y * image_height\n",
    "        coco_keypoints.extend([x, y])  \n",
    "    \n",
    "    keypoints = torch.tensor(coco_keypoints, dtype=torch.float32)\n",
    "    return keypoints\n",
    "\n",
    "def predict_position(model, keypoints1, keypoints2=None):\n",
    "    if keypoints1 is None:\n",
    "        return None, 0.0\n",
    "    \n",
    "    if keypoints2 is None:\n",
    "        keypoints2 = torch.zeros_like(keypoints1)\n",
    "    \n",
    "    batch_input = torch.stack([keypoints1, keypoints2]).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(batch_input)\n",
    "        probs = torch.nn.functional.softmax(output, dim=1)\n",
    "        confidence, prediction = torch.max(probs, 1)\n",
    "    \n",
    "    return POSITION_CLASSES[prediction.item()], confidence.item()\n",
    "\n",
    "def draw_pose(image, pose_landmarks, mp_pose, mp_drawing, color=(0, 255, 0)):\n",
    "    if pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 128, 255), thickness=2)\n",
    "        )\n",
    "    return image\n",
    "\n",
    "def setup_prediction_smoothing(window_size=5):\n",
    "    return deque(maxlen=window_size)\n",
    "\n",
    "def get_smoothed_prediction(prediction_history):\n",
    "    if not prediction_history:\n",
    "        return None, 0.0\n",
    "    \n",
    "    pred_counts = {}\n",
    "    conf_sums = {}\n",
    "    \n",
    "    for pred, conf in prediction_history:\n",
    "        if pred not in pred_counts:\n",
    "            pred_counts[pred] = 0\n",
    "            conf_sums[pred] = 0\n",
    "        pred_counts[pred] += 1\n",
    "        conf_sums[pred] += conf\n",
    "    \n",
    "    max_count = 0\n",
    "    smoothed_pred = None\n",
    "    \n",
    "    for pred, count in pred_counts.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            smoothed_pred = pred\n",
    "    \n",
    "    avg_conf = conf_sums[smoothed_pred] / pred_counts[smoothed_pred] if smoothed_pred else 0\n",
    "    \n",
    "    return smoothed_pred, avg_conf\n",
    "\n",
    "def run_multi_person_detection(model_path='pose_transformer.pth'):\n",
    "    model = load_model(model_path)\n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # media pipe detect one person at a time, so run it do daffa with different ROI's fucker\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        enable_segmentation=True,  \n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    prediction_history = setup_prediction_smoothing()\n",
    "    \n",
    "    prev_time = 0\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to read from webcam.\")\n",
    "                break\n",
    "            \n",
    "            current_time = time.time()\n",
    "            fps = 1 / (current_time - prev_time) if prev_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "            \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_height, image_width, _ = image.shape\n",
    "            \n",
    "            results = pose.process(image_rgb)\n",
    "            \n",
    "            keypoints1 = None\n",
    "            keypoints2 = None\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                keypoints1 = preprocess_keypoints(results.pose_landmarks.landmark, image_height, image_width)\n",
    "                \n",
    "                image = draw_pose(image, results.pose_landmarks, mp_pose, mp_drawing, color=(0, 255, 0))\n",
    "                \n",
    "                if results.segmentation_mask is not None:\n",
    "                    mask = np.ones((image_height, image_width), dtype=np.uint8) * 255\n",
    "                    segmentation_mask = results.segmentation_mask > 0.5\n",
    "                    mask[segmentation_mask] = 0\n",
    "                    \n",
    "                    masked_image = image_rgb.copy()\n",
    "                    for c in range(3):\n",
    "                        masked_image[:, :, c] = cv2.bitwise_and(masked_image[:, :, c], mask)\n",
    "                    \n",
    "                    second_results = pose.process(masked_image)\n",
    "                    \n",
    "                    if second_results.pose_landmarks:\n",
    "                        keypoints2 = preprocess_keypoints(second_results.pose_landmarks.landmark, image_height, image_width)\n",
    "                        \n",
    "                        image = draw_pose(image, second_results.pose_landmarks, mp_pose, mp_drawing, color=(255, 0, 0))\n",
    "            \n",
    "            position, confidence = None, 0.0\n",
    "            if keypoints1 is not None:\n",
    "                position, confidence = predict_position(model, keypoints1, keypoints2)\n",
    "                prediction_history.append((position, confidence))\n",
    "                smoothed_position, smoothed_confidence = get_smoothed_prediction(prediction_history)\n",
    "            else:\n",
    "                smoothed_position, smoothed_confidence = None, 0.0\n",
    "            \n",
    "            prediction_text = f\"Position: {smoothed_position if smoothed_position else 'None'}\"\n",
    "            confidence_text = f\"Confidence: {smoothed_confidence:.2f}\"\n",
    "            fps_text = f\"FPS: {fps:.1f}\"\n",
    "            person_text = f\"People detected: {(1 if keypoints1 is not None else 0) + (1 if keypoints2 is not None else 0)}\"\n",
    "            \n",
    "            cv2.putText(image, prediction_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, confidence_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, fps_text, (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, person_text, (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('BJJ Position Recognition', image)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Detection stopped by user\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def run_multi_person_detection_with_holistic(model_path='pose_transformer.pth'):\n",
    "    \"\"\"\n",
    "    Alternative approach using MediaPipe's Holistic model\n",
    "    combined with a custom algorithm to separate two people\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    holistic = mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    prediction_history = setup_prediction_smoothing()\n",
    "    \n",
    "    prev_time = 0\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to read from webcam.\")\n",
    "                break\n",
    "            \n",
    "            current_time = time.time()\n",
    "            fps = 1 / (current_time - prev_time) if prev_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "            \n",
    "            image = cv2.flip(image, 1)\n",
    "            \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_height, image_width, _ = image.shape\n",
    "            \n",
    "            results = holistic.process(image_rgb)\n",
    "            \n",
    "           \n",
    "            person1_landmarks = results.pose_landmarks\n",
    "            \n",
    "            keypoints1 = None\n",
    "            keypoints2 = None\n",
    "            \n",
    "            if person1_landmarks:\n",
    "                keypoints1 = preprocess_keypoints(person1_landmarks.landmark, image_height, image_width)\n",
    "                image = draw_pose(image, person1_landmarks, mp_pose, mp_drawing, color=(0, 255, 0))\n",
    "                \n",
    "                \n",
    "                second_image = image_rgb.copy()\n",
    "                \n",
    "                landmarks = np.array([[lm.x * image_width, lm.y * image_height] for lm in person1_landmarks.landmark])\n",
    "                x_min, y_min = np.min(landmarks, axis=0).astype(int)\n",
    "                x_max, y_max = np.max(landmarks, axis=0).astype(int)\n",
    "                \n",
    "                padding = 50\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(image_width, x_max + padding)\n",
    "                y_max = min(image_height, y_max + padding)\n",
    "                \n",
    "                mask = np.ones((image_height, image_width), dtype=np.uint8) * 255\n",
    "                mask[y_min:y_max, x_min:x_max] = 0\n",
    "                \n",
    "                for c in range(3):\n",
    "                    second_image[:, :, c] = cv2.bitwise_and(second_image[:, :, c], mask)\n",
    "                \n",
    "                second_results = holistic.process(second_image)\n",
    "                \n",
    "                if second_results.pose_landmarks:\n",
    "                    keypoints2 = preprocess_keypoints(second_results.pose_landmarks.landmark, image_height, image_width)\n",
    "                    image = draw_pose(image, second_results.pose_landmarks, mp_pose, mp_drawing, color=(255, 0, 0))\n",
    "            \n",
    "            position, confidence = None, 0.0\n",
    "            if keypoints1 is not None:\n",
    "                position, confidence = predict_position(model, keypoints1, keypoints2)\n",
    "                prediction_history.append((position, confidence))\n",
    "                smoothed_position, smoothed_confidence = get_smoothed_prediction(prediction_history)\n",
    "            else:\n",
    "                smoothed_position, smoothed_confidence = None, 0.0\n",
    "            \n",
    "            # Add prediction text\n",
    "            prediction_text = f\"Position: {smoothed_position if smoothed_position else 'None'}\"\n",
    "            confidence_text = f\"Confidence: {smoothed_confidence:.2f}\"\n",
    "            fps_text = f\"FPS: {fps:.1f}\"\n",
    "            person_text = f\"People detected: {(1 if keypoints1 is not None else 0) + (1 if keypoints2 is not None else 0)}\"\n",
    "            \n",
    "            cv2.putText(image, prediction_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, confidence_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, fps_text, (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, person_text, (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('BJJ Position Recognition', image)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Detection stopped by user\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def run_optimized_multi_person_detection(model_path='pose_transformer.pth'):\n",
    "    \"\"\"\n",
    "    Most robust implementation for two-person BJJ position detection\n",
    "    using MediaPipe Pose with custom tracking for multiple people\n",
    "    \"\"\"\n",
    "    model = load_model(model_path)\n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    pose1 = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    pose2 = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        smooth_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    prediction_history = setup_prediction_smoothing()\n",
    "    \n",
    "    prev_time = 0\n",
    "    \n",
    "    person1_center = None\n",
    "    person2_center = None\n",
    "    person1_tracked = False\n",
    "    person2_tracked = False\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to read from webcam.\")\n",
    "                break\n",
    "            \n",
    "            current_time = time.time()\n",
    "            fps = 1 / (current_time - prev_time) if prev_time > 0 else 0\n",
    "            prev_time = current_time\n",
    "            \n",
    "            image_height, image_width, _ = image.shape\n",
    "            \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            results1 = pose1.process(image_rgb)\n",
    "            \n",
    "            keypoints1 = None\n",
    "            keypoints2 = None\n",
    "            \n",
    "            if results1.pose_landmarks:\n",
    "                keypoints1 = preprocess_keypoints(results1.pose_landmarks.landmark, image_height, image_width)\n",
    "                \n",
    "                image = draw_pose(image, results1.pose_landmarks, mp_pose, mp_drawing, color=(0, 255, 0))\n",
    "                \n",
    "                landmarks = np.array([[lm.x * image_width, lm.y * image_height] for lm in results1.pose_landmarks.landmark])\n",
    "                center1 = np.mean(landmarks, axis=0)\n",
    "                person1_center = center1\n",
    "                person1_tracked = True\n",
    "                \n",
    "                mask = np.ones((image_height, image_width), dtype=np.uint8) * 255\n",
    "                \n",
    "                x_min, y_min = np.min(landmarks, axis=0).astype(int)\n",
    "                x_max, y_max = np.max(landmarks, axis=0).astype(int)\n",
    "                \n",
    "                padding = 30\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(image_width, x_max + padding)\n",
    "                y_max = min(image_height, y_max + padding)\n",
    "                \n",
    "                mask[y_min:y_max, x_min:x_max] = 0\n",
    "                masked_image = image_rgb.copy()\n",
    "                for c in range(3):\n",
    "                    masked_image[:, :, c] = cv2.bitwise_and(masked_image[:, :, c], mask)\n",
    "                \n",
    "                results2 = pose2.process(masked_image)\n",
    "                \n",
    "                if results2.pose_landmarks:\n",
    "                    keypoints2 = preprocess_keypoints(results2.pose_landmarks.landmark, image_height, image_width)\n",
    "                    \n",
    "                    image = draw_pose(image, results2.pose_landmarks, mp_pose, mp_drawing, color=(255, 0, 0))\n",
    "                    \n",
    "                    landmarks2 = np.array([[lm.x * image_width, lm.y * image_height] for lm in results2.pose_landmarks.landmark])\n",
    "                    center2 = np.mean(landmarks2, axis=0)\n",
    "                    person2_center = center2\n",
    "                    person2_tracked = True\n",
    "            \n",
    "            position, confidence = None, 0.0\n",
    "            if keypoints1 is not None:\n",
    "                position, confidence = predict_position(model, keypoints1, keypoints2)\n",
    "                prediction_history.append((position, confidence))\n",
    "                smoothed_position, smoothed_confidence = get_smoothed_prediction(prediction_history)\n",
    "            else:\n",
    "                smoothed_position, smoothed_confidence = None, 0.0\n",
    "            \n",
    "            prediction_text = f\"Position: {smoothed_position if smoothed_position else 'None'}\"\n",
    "            confidence_text = f\"Confidence: {smoothed_confidence:.2f}\"\n",
    "            fps_text = f\"FPS: {fps:.1f}\"\n",
    "            person_text = f\"People detected: {(1 if keypoints1 is not None else 0) + (1 if keypoints2 is not None else 0)}\"\n",
    "            \n",
    "            cv2.putText(image, prediction_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, confidence_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, fps_text, (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(image, person_text, (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('BJJ Position Recognition', image)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Detection stopped by user\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting BJJ position detection with multi-person support...\")\n",
    "    print(\"Choose detection method:\")\n",
    "    print(\"1: Basic Multi-Person Detection\")\n",
    "    print(\"2: Holistic-based Multi-Person Detection\")\n",
    "    print(\"3: Optimized Multi-Person Detection (Recommended)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1/2/3): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        run_multi_person_detection()\n",
    "    elif choice == '2':\n",
    "        run_multi_person_detection_with_holistic()\n",
    "    else:\n",
    "        run_optimized_multi_person_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
